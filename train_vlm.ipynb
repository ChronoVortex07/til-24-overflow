{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import yaml\n",
    "import shutil\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLOWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gatastol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_caption(caption):\n",
    "    # This function cleans the caption by removing stopwords and punctuations.\n",
    "    caption = re.sub(r'[^\\w\\s]', '', caption)\n",
    "    word_list = [word for word in caption.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def create_inferior_labels(caption):\n",
    "    # This function generates multiple inferior versions of a label by having different permutations of reduced adjectives for the label.\n",
    "    # For example, a red cargo airplane can be labeled as a red cargo airplane, a red airplane, a cargo airplane, and an airplane.\n",
    "    # remove stopwords and punctuations\n",
    "    caption = re.sub(r'[^\\w\\s]', '', caption)\n",
    "    word_list = [word for word in caption.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "    # get all possible combinations of words\n",
    "    inferior_labels = []\n",
    "    for i in range(len(word_list), 0, -1):\n",
    "        for subset in itertools.combinations(word_list, i):\n",
    "            inferior_labels.append(' '.join(subset))\n",
    "    \n",
    "            \n",
    "    return inferior_labels\n",
    "\n",
    "# print(create_inferior_labels('blue, yellow, and white cargo aircraft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gatastol/Documents/GitHub/til-24-overflow/.venv/lib/python3.10/site-packages/albumentations/core/validation.py:34: UserWarning: Argument 'var_limit' is not valid and will be ignored.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "input_dir = Path(\"data\")\n",
    "tables = []\n",
    "counter = 0\n",
    "num_train_files = int(5107 * 0.95 *5)#*0.8\n",
    "\n",
    "yolo_root = Path(\"data/yolo\")\n",
    "\n",
    "image_dir = yolo_root / \"images\"\n",
    "label_dir = yolo_root / \"labels\"\n",
    "\n",
    "image_train_dir = image_dir / \"train\"\n",
    "image_val_dir = image_dir / \"val\"\n",
    "label_train_dir = label_dir / \"train\"\n",
    "label_val_dir = label_dir / \"val\"\n",
    "shutil.rmtree(image_dir, ignore_errors=True)\n",
    "shutil.rmtree(label_dir, ignore_errors=True)\n",
    "image_train_dir.mkdir(parents=True, exist_ok=True)\n",
    "image_val_dir.mkdir(parents=True, exist_ok=True)\n",
    "label_train_dir.mkdir(parents=True, exist_ok=True)\n",
    "label_val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels = []\n",
    "\n",
    "tranform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(p=1, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "    A.RandomBrightnessContrast(p=1),\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    A.GaussNoise(p=1, var_limit=(0.0, 400.0), per_channel=True),\n",
    "    A.CLAHE(p=0.5),\n",
    "    A.ImageCompression(quality_lower=75, p=0.2),\n",
    "    A.RandomRain(p=0.1),\n",
    "    A.RandomFog(p=0.1),\n",
    "    A.MultiplicativeNoise(p=1, var_limit=(0.0, 400.0), per_channel=True),\n",
    "    A.RandomSunFlare(p=0.1),\n",
    "    A.GridDistortion(p=0.2)\n",
    "    #A.GridDistortion(p=0.0),\n",
    "    # A.RandomCrop(width=640, height=640, p=0.5),\n",
    "    # A.ShiftScaleRotate(p=1),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.VerticalFlip(p=0.5),\n",
    "    # A.RandomBrightnessContrast(p=1),\n",
    "    # A.HueSaturationValue(p=1),\n",
    "    # A.AdvancedBlur(blur_limit=3, p=0.5),\n",
    "    # A.RandomRain(p=0.5),\n",
    "    # A.GaussNoise(p=1, var_limit=(10.0, 150.0), per_channel=True),\n",
    "    # A.ISONoise(p=1, intensity=(0.1, 0.5), color_shift=(0.01, 0.05)),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['captions'], min_visibility=0.2))\n",
    "\n",
    "for i in range(3,8):\n",
    "    with open(input_dir / \"vlm.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            \n",
    "            sub_dir = \"train\" if counter < num_train_files else \"val\"\n",
    "            \n",
    "            instance = json.loads(line.strip())\n",
    "            image = cv2.imread(str(input_dir / \"images\" / instance[\"image\"]))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            img_h, img_w, _ = image.shape\n",
    "            \n",
    "            bboxes = []\n",
    "            captions = []\n",
    "            for annotation in instance[\"annotations\"]:\n",
    "                caption = clean_caption(annotation[\"caption\"]) #'/'.join(create_inferior_labels(annotation[\"caption\"]))\n",
    "                x, y, w, h = annotation[\"bbox\"]\n",
    "                if caption not in labels:\n",
    "                    labels.append(caption)\n",
    "                captions.append(caption)\n",
    "                yolo_x = (x + w/2) / img_w\n",
    "                yolo_y = (y + h/2) / img_h\n",
    "                yolo_w = w / img_w\n",
    "                yolo_h = h / img_h\n",
    "                bboxes.append([yolo_x, yolo_y, yolo_w, yolo_h])\n",
    "            transformed = tranform(image=image, captions=captions, bboxes=bboxes)\n",
    "            image = transformed[\"image\"]\n",
    "            bboxes = transformed[\"bboxes\"]\n",
    "            captions = transformed[\"captions\"]\n",
    "\n",
    "\n",
    "            cv2.imwrite(str(image_dir / sub_dir / instance[\"image\"].replace(\".jpg\", \"-\"+str(i)+\".jpg\")), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "            with open(label_dir / sub_dir / instance[\"image\"].replace(\".jpg\", \"-\"+str(i)+\".txt\"), \"w\") as label_file:\n",
    "                for caption, bbox in zip(captions, bboxes):\n",
    "                    yolo_x, yolo_y, yolo_w, yolo_h = bbox\n",
    "                    label_file.write(f\"{labels.index(caption)} {yolo_x} {yolo_y} {yolo_w} {yolo_h}\\n\")\n",
    "                    # cv2.rectangle(image, (int((yolo_x - yolo_w/2) * img_w), int((yolo_y - yolo_h/2) * img_h), int(yolo_w * img_w), int(yolo_h * img_h)), (0, 255, 0), 2)\n",
    "                            \n",
    "            # plt.imshow(image)>\n",
    "            # plt.show()\n",
    "            # break\n",
    "        \n",
    "            counter += 1\n",
    "        \n",
    "# write yaml file\n",
    "with open(yolo_root / \"dataset_open.yaml\", \"w\") as f:\n",
    "    yaml.dump({\n",
    "        \"names\": dict(enumerate(labels)),\n",
    "        \"val\": \"images/val\",\n",
    "        \"train\": \"images/train\",\n",
    "        \"path\": os.path.abspath(yolo_root),\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics.data.augment import Albumentations\n",
    "# from ultralytics.utils import LOGGER, colorstr\n",
    "\n",
    "# def __init__(self, p=1.0):\n",
    "#     \"\"\"Initialize the transform object for YOLO bbox formatted params.\"\"\"\n",
    "#     self.p = p\n",
    "#     self.transform = None\n",
    "#     prefix = colorstr(\"albumentations: \")\n",
    "#     try:\n",
    "#         import albumentations as A\n",
    "\n",
    "#         # Define your desired transformations here\n",
    "#         T = [\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             A.VerticalFlip(p=0.5),\n",
    "#             A.ShiftScaleRotate(p=1),\n",
    "#             A.RandomBrightnessContrast(p=1),\n",
    "#             A.AdvancedBlur(blur_limit=3, p=0.2),\n",
    "#             A.GaussNoise(p=1, var_limit=(0.0, 300.0), per_channel=True),\n",
    "#             A.CLAHE(p=0.5),\n",
    "#             A.ImageCompression(quality_lower=75, p=0.2),\n",
    "#             A.RandomRain(p=0.1),\n",
    "#             A.RandomFog(p=0.1),\n",
    "#             A.MultiplicativeNoise(p=1, var_limit=(0.0, 300.0), per_channel=True),\n",
    "#             A.RandomSunFlare(p=0.1),\n",
    "#             A.grid_distortion(0.2)\n",
    "#         ]\n",
    "\n",
    "#         self.transform = A.Compose(T, bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]))\n",
    "#         LOGGER.info(prefix + \", \".join(f\"{x}\".replace(\"always_apply=False, \", \"\") for x in T if x.p))\n",
    "#     except ImportError:\n",
    "#         # Albumentations package not installed, skip\n",
    "#         pass\n",
    "#     except Exception as e:\n",
    "#         LOGGER.info(f\"{prefix}{e}\")\n",
    "\n",
    "# Albumentations.__init__ = __init__last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained model\n",
    "model = YOLOWorld('data/yolo/vlm_album_large/weights/best.pt') # 'data/yolo/vlm_yolo_album_large/weights/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.25 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.18 ðŸš€ Python-3.10.12 torch-2.0.1+rocm5.4.2 CUDA:0 (AMD Radeon RX 6800 XT, 16368MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=data/yolo/vlm_album_large/weights/last.pt, data=data/yolo/dataset_open.yaml, epochs=1, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=data/yolo, name=vlm_album_large, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=data/yolo/vlm_album_large\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   5576456  ultralytics.nn.modules.block.C2fAttn         [1024, 512, 3, 256, 8]        \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1493892  ultralytics.nn.modules.block.C2fAttn         [768, 256, 3, 128, 4]         \n",
      " 16                  15  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   5445384  ultralytics.nn.modules.block.C2fAttn         [768, 512, 3, 256, 8]         \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   5576456  ultralytics.nn.modules.block.C2fAttn         [1024, 512, 3, 256, 8]        \n",
      " 22        [15, 18, 21]  1   5980630  ultralytics.nn.modules.head.WorldDetect      [80, 512, True, [256, 512, 512]]\n",
      "YOLOv8l-worldv2 summary: 396 layers, 46832050 parameters, 46832034 gradients, 204.5 GFLOPs\n",
      "\n",
      "Transferred 652/652 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/gatastol/Documents/GitHub/til-24-overflow/data/yolo/labels/train.cache... 38046 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38046/38046 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gatastol/Documents/GitHub/til-24-overflow/.venv/lib/python3.10/site-packages/albumentations/core/composition.py:151: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  warnings.warn(f\"Got processor for {proc.default_data_name}, but no transform to process it.\")\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/gatastol/Documents/GitHub/til-24-overflow/data/yolo/labels/val.cache... 2810 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2810/2810 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to data/yolo/vlm_album_large/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=7.7e-05, momentum=0.9) with parameter groups 104 weight(decay=0.0), 115 weight(decay=0.0005), 121 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mdata/yolo/vlm_album_large\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      5.98G      1.214      1.044       0.97         71        640:   9%|â–‰         | 442/4756 [04:29<43:48,  1.64it/s]"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    data=\"data/yolo/dataset_open.yaml\",\n",
    "    epochs=1,\n",
    "    batch=8,\n",
    "    imgsz=640,\n",
    "    device=0,\n",
    "    #save_period=1,\n",
    "    project=\"data/yolo\",\n",
    "    name=\"vlm_album_large\",\n",
    "    exist_ok=True,\n",
    "    verbose=True,\n",
    "    seed=0,\n",
    "    #patience=10,\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.18 ðŸš€ Python-3.10.12 torch-2.0.1+rocm5.4.2 CUDA:0 (AMD Radeon Graphics, 512MiB)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# test model on validation set\n",
    "model.val(\n",
    "    data=\"data/yolo/dataset_open.yaml\", \n",
    "    batch=8, \n",
    "    imgsz=640, \n",
    "    device=0, \n",
    "    verbose=True,\n",
    "    project=\"data/yolo\",\n",
    "    name=\"vlm_album_large\",\n",
    "    exist_ok=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
